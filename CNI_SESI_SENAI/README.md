# Avalia√ß√£o T√©cnica 123 ‚Äì Engenheiro de Dados
### **Nome do Candidato:** Willdeglan de Sousa Santos
### **Data:** 02/09/2025   [**E-mail**](willdeglan@gmail.com)
---

## **Parte 1 ‚Äì Quest√µes de M√∫ltipla Escolha**

### 1. Sobre a arquitetura em camadas (Bronze, Silver e Gold), qual das op√ß√µes descreve corretamente suas fun√ß√µes?
- [ ] A. Bronze: dados limpos; Silver: dados crus; Gold: dados agregados\
- [ ] B. Bronze: dados estruturados; Silver: dados n√£o estruturados; Gold: dados arquivados\
- [x] C. Bronze: ingest√£o de dados crus; Silver: dados refinados e com qualidade; Gold: dados prontos para consumo anal√≠tico\
- [ ] D. Bronze: dados replicados; Silver: dados eliminados; Gold: dados transformados em imagens\
- [ ] E. Bronze: arquivos tempor√°rios; Silver: dados de machine learning; Gold: dados brutos


### 2. Durante o desenvolvimento de pipelines de dados, qual das pr√°ticas abaixo est√° mais alinhada com princ√≠pios de engenharia de dados escal√°vel e sustent√°vel?
- [ ] A. Realizar transforma√ß√µes diretamente na camada de apresenta√ß√£o para reduzir lat√™ncia
- [ ] B. Evitar versionamento de dados para reduzir espa√ßo em disco
- [ ] C. Centralizar toda a l√≥gica de neg√≥cios em dashboards anal√≠ticos
- [x] D. Separar responsabilidades em etapas reutiliz√°veis e com controle de versionamento
- [ ] E. Construir pipelines √∫nicos com l√≥gica acoplada e parametriza√ß√£o m√≠nima

### 3. Em um processo de ingest√£o de dados de fontes externas, qual pr√°tica contribui para maior resili√™ncia e confiabilidade do pipeline?
- [ ] A. Executar a carga manualmente para garantir precis√£o
- [ ] B. Evitar logs para n√£o gerar arquivos desnecess√°rios
- [x] C. Implementar mecanismos de monitoramento, retry e logging para falhas
- [ ] D. Eliminar checkpoints para reduzir a complexidade do c√≥digo
- [ ] E. Utilizar m√∫ltiplas fontes simultaneamente, sem controle de concorr√™ncia

### 4. Qual das op√ß√µes a seguir representa uma vantagem do uso de formatos de dados orientados a colunas (como Parquet ou ORC) em ambientes anal√≠ticos?
- [x] A. Melhor desempenho em leitura seletiva de colunas e compress√£o eficiente
- [ ] B. Facilidade para edi√ß√µes linha a linha em arquivos
- [ ] C. Compatibilidade com arquivos XML sem necessidade de convers√£o
- [ ] D. Estrutura√ß√£o ideal para uso com bancos de dados relacionais
- [ ] E. Suporte autom√°tico √† normaliza√ß√£o de dados

### 5. Em rela√ß√£o ao conceito de data lineage, qual das alternativas melhor descreve sua utilidade em um ambiente de dados?
- [ ] A. Minimizar o uso de metadados em ambientes de produ√ß√£o
- [ ] B. Automatizar a modelagem relacional dos dados
- [x] C. Mapear a origem, transforma√ß√£o e destino dos dados para auditoria e governan√ßa
- [ ] D. Otimizar diretamente a performance de consultas em camadas anal√≠ticas
- [ ] E. Armazenar logs de acesso a dashboards para m√©tricas de uso

 
## Parte 2 ‚Äì Quest√£o Discursiva
```
Explique, com suas pr√≥prias palavras, como voc√™ estruturaria um pipeline de dados em arquitetura
do tipo "medalh√£o", considerando todas as etapas de:
‚Ä¢	Extra√ß√£o de dados (batch e streaming, se aplic√°vel)
‚Ä¢	Transforma√ß√£o e limpeza dos dados
‚Ä¢	Armazenamento em camadas (Bronze, Silver e Gold)
‚Ä¢	Orquestra√ß√£o do processo
‚Ä¢	Disponibiliza√ß√£o dos dados para consumo (API, dashboards, relat√≥rios, camadas anal√≠ticas)

Utilize exemplos pr√°ticos e mencione os principais componentes. A resposta deve ter no m√°ximo uma p√°gina.

Em projetos pessoais que desenvolvo, foco em organiza√ß√£o como arquitetura medalh√£o assim o projeto
fica organizado, rastre√°vel e tem uma boa qualidade no fluxo de dados 
```
1.	**Extra√ß√£o de dados:** \
   pode ser por dois (2) caminhos: Batch ou Streaming 
‚Ä¢	Batch: A raspagem ou ingest√£o de dados s√£o carregados em lotes ou arquivos (CSV, JSON ou Planilhas) para a Silver ou pode armazenar em um storage (raw) e inserir em tabelas do formato original (cru).
‚Ä¢	Streaming: quando necess√°rio, √© utilizado conectores para consumir dados em tempo real (por exemplo, Kafka ou Event Hub no Azure).

2.	Transforma√ß√£o e limpeza dados: \
   depois dos arquivos obtidos na extra√ß√£o, √© realizar a padroniza√ß√£o (tipagem dos dados, normaliza√ß√£o ou ‚Äúdesnormaliza√ß√£o‚Äù, corre√ß√£o de colunas) tratar as inconsist√™ncias e imperfei√ß√µes como duplicatas, nulos ou dados inconsistentes.

3.	Armazenamento em camadas: \
‚Ä¢	BRONZE: datasets crus, como obtidos, podem ser organizados por data de carga, ou at√© particionado. \
‚Ä¢	SILVER: j√° come√ßa a ser tratado, agrupado, padronizado e at√© enriquecido, eliminando dados sujos ou incompletos.\
‚Ä¢	 GOLD: dados pronto para ser consumido, disponibilizado como StarSchema, metricas j√° calculadas e agrega√ß√µes.

4.	Orquestra√ß√£o: \
   No Databricks Free Edition, por exemplo, de forma automatizada, organizo um c√≥digo python para cada etapa (1_bronze, 2_silver, 3_gold) e agendo a execu√ß√£o em sequ√™ncia. Em ambientes corporativos, ferramentas como Airflow, Databricks Jobs e ADF fazem o monitoramento e alertas em caso de falhas.

5.	Disponibiliza√ß√£o: \
   para consumo na Gold os dados podem ser disponibilizados de algumas formas: \
‚Ä¢	APIs: para integra√ß√£o com aplica√ß√£o \
‚Ä¢	DASHBOARD: utilizando BI \
‚Ä¢	CONSULTAS ANAL√çTICAS: possibilitando ser consumido em relat√≥rios com SQL
‚ÄÉ
## Parte 3 ‚Äì Desafio Pr√°tico ‚Äì Constru√ß√£o de Bot de Dados
```
1. Desenvolva um bot (rob√¥) em Python que atenda aos crit√©rios abaixo:
‚Ä¢	Utilize a linguagem python (vers√£o 3 ou superior)
‚Ä¢	Obtenha os dados do IPCA em: https://sidra.ibge.gov.br/Ajax/JSon/Tabela/1/1737?versao=-1
‚Ä¢	Coloque os dados no formato tabular (estruturado) e grave um arquivo com este conte√∫do no formato ‚Äúparquet‚Äù
‚Ä¢	Construa ao menos 3 fun√ß√µes (ou m√©todos) e as utilize no c√≥digo
‚Ä¢	Documente as etapas do processo dentro do pr√≥prio c√≥digo
‚Ä¢	Disponibilize o(s) c√≥digo(s) e o arquivo final gerado pelo bot (parquet) em um projeto do
    GitHub (reposit√≥rio p√∫blico - https://github.com/)

```

### Repositorio no github com o desafio:
https://github.com/willdegl4n/Projetos/tree/main/CNI_SESI_SENAI

Arquivo: Bot_IPCA.py
```python
"""
Bot de Dados - IPCA (IBGE / SIDRA)
Autor: Willdeglan do SQL Dicas
Descri√ß√£o:
    Este script consome dados do IPCA no formato JSON, organiza em tabela,
    e salva no formato Parquet para consumo anal√≠tico.

No databricks n√£o precisa rodar esse comando, mas se for rodar o codigo 
fora do databricks, √© necess√°rio rodar o seguinte comando:
 
    %pip install -U requests pyarrow
"""


import pandas as pd
from pathlib import Path # <--- para criar pastas de sa√≠da com seguran√ßa (output)
import requests  # <--- necess√°rio para acessar a URL
import json # <--- necess√°rio para acessar JSON local

# =========================
# Fun√ß√£o 1 - Ler dados JSON
# =========================
"""Carrega o JSON bruto do IPCA (arquivo local ou URL)"""
def carregar_dados_json(origem: str) -> dict:
    if origem.startswith("http"):
        response = requests.get(origem)
        response.raise_for_status() # informa o codigo do erro, se houver
        return response.json()
    else: # caso a origem nao seja uma URL, ser√° lido essa parte
        with open(origem, "r", encoding="utf-8") as f:
            return json.load(f)


# ===================================
# Fun√ß√£o 2 - Transformar para tabular
# ===================================
"""Transforma os per√≠odos do JSON em DataFrame pandas, se precisar ele adapta novas colunas automaticamente"""
def transformar_em_tabela(dados_json: dict) -> pd.DataFrame:
    periodos = dados_json["Periodos"]["Periodos"]

    df = pd.DataFrame(periodos)

    # dicion√°rio de renomea√ß√£o (quando a coluna for conhecida) no padr√£o snake_case
    rename_map = {
        "Id": "id",
        "Codigo": "codigo",
        "Nome": "mes_referencia",
        "Disponivel": "disponivel",
        "DataLiberacao": "data_liberacao"
    }

    df = df.rename(columns=rename_map)

    return df

# ====================================
# Fun√ß√£o 3 - Salvar em formato Parquet
# ====================================
"""Salva os dados tabulares em formato parquet"""
def salvar_parquet(df: pd.DataFrame, caminho_saida: str) -> None:

    Path(caminho_saida).parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(caminho_saida, engine="pyarrow", index=False)

# =========================
# Execu√ß√£o principal do bot
# =========================
if __name__ == "__main__":
    # Etapa 1: Carregar dados (arquivo local OU request da API)
    url = "https://sidra.ibge.gov.br/Ajax/JSon/Tabela/1/1737?versao=-1"
    dados = carregar_dados_json(url)

    # Etapa 2: Transformar em tabela
    df_ipca = transformar_em_tabela(dados)

    # Etapa 3: Salvar parquet
    salvar_parquet(df_ipca, "parquet/ipca.parquet")

# =============================
# Exibindo as informa√ß√µes do DF
# =============================
print("‚úÖ Bot rodando com sucesso. Arquivo salvo em 'parquet/ipca.parquet'")
print(df_ipca.head())  # Mostra as primeiras 5 linhas
print(f"N√∫mero de linhas: {len(df_ipca)}")  # faz a contagem das linhas

```
```
2. No exemplo do bot que voc√™ construiu, o conjunto de dados necess√°rios estava dispon√≠vel no site diretamente
por meio de um link j√° definido. No entanto como voc√™ resolveria o problema da captura dos dados caso fosse
necess√°rio antes navegar no site (executando passos e cliques por meio de menus, login, bot√µes, links) para
se chegar ao arquivo alvo (se n√£o existisse um link direto para o conjunto de dados).

Como resposta para este item, crie um arquivo (txt), disponibilize-o no referido projeto do GitHub e inclua
no seu conte√∫do um texto explicativo (com suas pr√≥prias palavras) que descreva uma proposta de solu√ß√£o.
```

Arquivo: resposta_parte2.txt
```text
Se caso a url com o dataset n√£o estivesse dispon√≠vel em um link direto, seria necess√°rio 
automatizar a navega√ß√£o at√© o arquivo alvo.

A solu√ß√£o possiveis √© utilizar bibliotecas de automa√ß√£o de navega√ß√£o, como 
* Selenium (Python) - https://www.youtube.com/watch?app=desktop&v=XLkxOBY965w
* Playwright - https://www.youtube.com/watch?v=yNuTu8csOU0

Com essas bibliotecas, conseguimos simular a intera√ß√£o humana em um navegador:
* login
* clique em menus e bot√µes ou 
* download de arquivos.

O processo seria estruturado em etapas:
1. Abrir o navegador de forma program√°tica (headless).
2. Executar login (se necess√°rio) enviando usu√°rio/senha.
3. Navegar pelos menus e links, simulando cliques.
4. Localizar o link de download ou fazer o download autom√°tico.
5. Assim que conseguir o dataset com os dados bruto (CSV/JSON), o pipeline segue normalmente:
   transforma√ß√£o > padroniza√ß√£o > armazenamento em parquet > ingest√£o na arquitetura Bronze

Esse processo vai garantir a automa√ß√£o e reprodutibilidade, tornando desnecess√°ria a interven√ß√£o 
manual e tornando o pipeline resiliente.
```

### *Willdeglan de S. Santos*
Data Engineer | DBA | Criador do SQL Dicas  
üîó LinkedIn: [@willdeglan](https://www.linkedin.com/in/willdeglan) | üìò [@sqldicas](https://www.linkedin.com/company/sqldicas)  
